---
title: "Bayesian Poisson Regression"
author: "Simon Thornewill von Essen"
date: "4 2 2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set

library("COUNT")
library("rjags")

data("badhealth")

set.seed(102)
```

In this document, we will go over how to do poisson regression.

For this regression we will be using the dataset `badhealth` which has 1127 observations with 3 columns. The number of visits to a doctor, an indicator variable stating whether the patient is in bad haelth and the age of the patient.

```{r}
hist(badhealth$numvisit, breaks=20)
```

We can see that the number of visits is exponentially distributed. We will need to take care of the zero values when using the log of this column.

```{r}
plot(jitter(log(numvisit)) ~ jitter(age), data=badhealth, subset=badh==0&numvisit>0, xlab="age", ylab="log(visits)")
points(jitter(log(numvisit)) ~ jitter(age), data=badhealth, subset=badh==1&numvisit>0, xlab="age", ylab="log(visits)", col="red")
```

It seems that being in bad health means more visits to the doctor.

Let's build the poisson regression

## Poisson Regression

```{r}
mod1_string <- "model{
  for(i in 1:length(numvisit)){
    numvisit[i] ~ dpois(lam[i])
    log(lam[i]) = int + b_badh*badh[i] + b_age*age[i] +
      b_intx*age[i]*badh[i]
  }
  
  int ~ dnorm(0.0, 1.0/1e6)
  b_badh ~ dnorm(0.0, 1.0/1e4)
  b_age ~ dnorm(0.0, 1.0/1e4)
  b_intx ~ dnorm(0.0, 1.0/1e4)
}"

data1_jags <- as.list(badhealth)

params1 <- c("int", "b_badh", "b_age", "b_intx")

mod1 <- jags.model(textConnection(mod1_string), data=data1_jags, n.chains=3)
update(mod1, 1e3)

mod1_sim <- coda.samples(model=mod1, variable.names=params1, n.iter=5e3)
mod1_csim <- as.mcmc(do.call(rbind, mod1_sim))

plot(mod1_sim)

# Remember convergence diagnostics
#    gelman.diag
#    autocorr.diag
#    autocorr.plot
#    effectiveSize
```

```{r}
dic1 <- dic.samples(mod1, n.iter=1e3)

dic1
```

Next, we can evaluate our results using residuals

```{r}
X <- as.matrix(badhealth[,-1])
X <- cbind(X, with(badhealth, badh*age))

head(X, 3)
tail(X, 3)
```

```{r}
pmed_coef <- apply(mod1_csim, 2, median)

pmed_coef
```

```{r}
llam_hat <- pmed_coef["int"] + X %*% pmed_coef[c("b_badh", "b_age", "b_intx")]
lam_hat <- exp(llam_hat)

resid <- badhealth$numvisit - lam_hat

plot(resid)
```

Normally, we would be alarmed by this residual plot. The data and the number of visits are not independant. But the data itself was presorted, so we don't need to worry about this.

```{r}
plot(lam_hat[which(badhealth$badh==0)], 
     resid[which(badhealth$badh==0)], 
     xlim=c(0, 8), ylim=c(range(resid)))
points(lam_hat[which(badhealth$badh==1)], 
     resid[which(badhealth$badh==1)], 
     xlim=c(0, 8),
     col="red")
```

We can see that the model predicted the number of visits to be roughly 2 or 6 depending on whether they were sick. Note as well that the variability increases as the prediction increases. (Remember that in the poiss distribution, the mean and the variance are the same.)

We can also have a look at variance to evaluate model fit

```{r}
var(resid[which(badhealth$badh==0)])
var(resid[which(badhealth$badh==1)])
```

We can see that our variance is much higher than expected, which shows that this model is a poor fit.

## Predictive distributions

```{r}
summary(mod1_sim)
```

The intercept refers to someone who is 0 years old, but the youngest person in this dataset is roughly 20, so we should not interpret this parameter. It seems that visits are slighly correlated with age but being in bad health is a much stronger predictor.

The interaction term between bad health and age is interpreted as the adjustment to age for people in bad health. It is negative and with a magnitude larger than age, which means that for someone in bad health the sign of the age coeff would switch from +ve to -ve.

Alright, but let's say we have two people of age 35 who are in good and bad health respectively. What is the posterior probability that the individual in poor health will have more doctor visits?

This is the first case where having a bayesian analysis pays off.

```{r}
# Create x values
x1 <- c(0, 35, 0*35)
x2 <- c(1, 35, 1*35)

# head(mod1_csim)

# Get monte carlo sample of the linear part of the posterior distribution
loglam1 <- mod1_csim[, "int"] + mod1_csim[, c(2, 1, 3)] %*% x1
loglam2 <- mod1_csim[, "int"] + mod1_csim[, c(2, 1, 3)] %*% x2

lam1 <- exp(loglam1)
lam2 <- exp(loglam2)
```

```{r}
plot(density(lam1))
```

Now, for the length of the simulation we have that many samples from the distribution of the parameters. We can use this to generate a bunch of observations that we can plot and do calculations with.

```{r}
n_sim <- length(lam1)

y1 <- rpois(n_sim, lam1)
y2 <- rpois(n_sim, lam2)

par(mfrow=c(2, 1))
plot(table(factor(y1, levels=0:18))/n_sim)
plot(table(factor(y2+0.1))/n_sim, col="red")
```

```{r}
mean(y2 > y1)
```

Now, at least we can see our uncertainity in our predictions as well as generating probailistic statements that might otherwise be impossible to make.
