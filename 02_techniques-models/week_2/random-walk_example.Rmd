---
title: "Random Walk Example"
author: "Simon Thornewill von Essen"
date: "4 12 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(coda)
```
In this document, we’ll code the metropolis-hastings algorithm for one of the intractable problems shown in the last 
week.

We are trying to estimate the percentage change of of total employees in 10 companies for the past year ($y_i$).

Our observed variable is normally distributed with a known variance. However, we have a prior on the mean which is t 
(cauchy) distributed. i.e.

$$y_i | \mu \sim N(\mu, 1), i = 1,...,n$$
$$\mu \sim t(0, 1, 1)$$

Our posterior distribution looks as such,

$$ p(\mu|y_1,...,y_n)  \propto \prod_{i=1}^n \left [\frac{1}{\sqrt{2\pi}}e^{\frac{-(y_i-\mu)^2}{2}} \right] \frac{1}{\pi(1+\mu^2)}$$

i.e. We have a product of observations that are I.I.D. normally distributed with a known stadard 
deviation of 1. We then need to mulitply by the prior, which is the $t(0, 1, 1)$

Note that since we have a product, we can think about looking at the log likelihood, which will 
turn the product into a sum.

We can also drop the proportionality constants since we are only looking at what our posterior is proportional to.

$$ e^{-.5 \sum(y_i-\mu)^2} * \frac{1}{1+\mu^2}$$

$$p(\mu|y_1,...,y_n)  \propto \frac{n(\bar{y}\mu - \frac{\mu^2}{2})}{1 + \mu^2}
$$

In order to make our calculations numerically stable, we will be sampling from the log-posterior distribution, i.e.

$$log[p(\mu|y_1,...,y_n)]  \propto n(\bar{y}\mu - \frac{\mu^2}{2}) - log(1 + \mu^2)$$

The equation above is our g(theta) for the metropolis hastings algorithm.

## Instantiate Functions

First, we need to create the log of g(theta) function that was outlined above…

```{r}
lg <- function(ybar, n, mu){
  mu2 <- mu^2
  n*(ybar*mu - mu2/2) - log(1 + mu2)
}

```

Then, we need to create out metropolis hastings function

```{r}
mh <- function(n, ybar, n_iter, mu_init, cand_sd) {
  mu_out <- numeric(n_iter)
  accpt <- 0
  mu_now <- mu_init
  lg_now <- lg(mu=mu_now, n=n, ybar=ybar)
  
  for (i in 1:n_iter) {
    # Note that our candidate distribution, q(theta), is normally distributed
    mu_cand <- rnorm(1, mean=mu_now, sd=cand_sd)
    
    lg_cand <- lg(mu=mu_cand, n=n, ybar=ybar)
    lalpha <- lg_cand - lg_now
    alpha <- exp(lalpha)
    
    u <- runif(1)  # Read this as rv from unif(orm)
    if (u < alpha){
      mu_now <- mu_cand
      accpt <- accpt + 1
      lg_now <- lg_cand
    }
    
    mu_out[i] = mu_now
  }
  
  list(mu=mu_out, accpt=accpt/n_iter)
}

```

## Run algorithm

Now that we have created out algorithm that we want to run, we need to apply it to our obserbed values

```{r}
y = c(1.2, 1.4, -0.5, 0.3, 0.9, 2.3, 1.0, 0.1, 1.3, 1.9)

ybar = mean(y)
n = length(y)
```

Now that we've defined the dataset we want to learn on, let's have a look at the histogram

```{r}
hist(y, freq=FALSE, xlim=c(-1.0, 3.0))
points(y, rep(0.0, n))
points(ybar, 0.0, pch=19)
curve(dt(x, df=1), lty=2, add=TRUE)

```

We can see above the histrogram of our observed values, y. We can also see with the dotted lines the curve for our prior 
distribution. Note that the prior has the majority of its probability density  around zero, where it should really be 
around 1.

Now that we have everything setup, it's time to start sampling from our posterior distribution.

```{r}
set.seed(42)
post <- mh(n=n, ybar=ybar, n_iter=1e3, mu_init = 0.0, cand_sd = 3.0)

str(post)
```

Now that we have sampled from our posterior distribution, it's time to have a look at our trace.

```{r}
traceplot(as.mcmc(post$mu))

```

We can see that our acceptance rate is relatively low at roughly 10% and that the trace has a hard time converging on 
the correct value for mu because the step size is too big.

Note that our candidate generating distribution is normally distributed, this is how we are
exploring the space. If we change the std. of this distribution then we change how quickly we
are searching through it.

```{r}
post <- mh(n=n, ybar=ybar, n_iter=1e3, mu_init = 0.0, cand_sd = 0.05)

traceplot(as.mcmc(post$mu))

```

Now we have the opposite problem, our estimation for mu wanders because the rate is too small.

```{r}
post <- mh(n=n, ybar=ybar, n_iter=1e3, mu_init = 0.0, cand_sd = 0.9)

traceplot(as.mcmc(post$mu))

```

We can see that this value works a lot better, we are exporing the posterior distribution without getting stuck at one
value for too long but also not wandering the posterior aimlessly.

What happens if we now take a value that is too far from the posterior distribution?

```{r}
post <- mh(n=n, ybar=ybar, n_iter=1e3, mu_init = 30, cand_sd = 0.9)

traceplot(as.mcmc(post$mu))

```

We can see that if we start with very unrealsitic values, we quickly correct ourselves to the correct one in roughly 100
iterations.

Now, for the post analysis. We have a bunch of samples in mu, we'll keep some as our random draws after the burn in 
period.

```{r}
post$mu_keep = post$mu[-c(1:100)]

plot(density(post$mu_keep))
curve(dt(x, df=1), lty=2, add=TRUE)
```
