---
title: "Gibbs Sampling Example"
author: "Simon Thornewill von Essen"
date: "12/21/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(coda)
```

Now, this notebook is about demonstrating how Gibbs Sampling works. We will be using the same example as in the last notebook demonstrating the metropolis hastings algorithm, namely, the example of the % change in company employees from one year to the next.

In the previous notebooks, the likelihood std. was fixed, but now we want to also have this as a parameter alongside the mean.

In one of the classes, we were able to find conjugate priors for the mean and std. which were Normal and Inverse Gamma respectively. 

## Write update functions

The first thing we need to do is write functions for the updates of $\mu$ and $\sigma^2$ accordingly.

```{r}
update_mu <- function(n, ybar, sig2, mu_0, sig2_0) {
  sig2_1 <- 1.0 / (n / sig2 + 1.0 / sig2_0)
  mu_1 <- sig2_1 * (n * ybar / sig2 + mu_0 / sig2_0)
  rnorm(n=1, mean=mu_1, sd=sqrt(sig2_1))
}

```

```{r}
update_sig2 <- function(n, y, mu, nu_0, beta_0) {
  nu_1 <- nu_0 + n / 2.0
  sumsq <- sum( (y - mu)^2 ) # vectorized
  beta_1 <- beta_0 + sumsq / 2.0
  out_gamma <- rgamma(n=1, shape=nu_1, rate=beta_1) # rate for gamma is shape for inv-gamma
  1.0 / out_gamma # reciprocal of a gamma random variable is distributed inv-gamma
}
```

## Gibbs Sampling

Now that we have written our update functions, it's time to write the Gibbs Sampling algorithm.

```{r}
gibbs <- function(y, n_iter, init, prior){
  ybar <- mean(y)
  n <- length(y)
  
  mu_out <- numeric(n_iter)
  sig2_out <- numeric(n_iter)
  
  mu_now <- init$mu
  
  ## Gibbs Sampler
  for(i in 1:n_iter){
    sig2_now <- update_sig2(n=n, y=y, mu=mu_now, nu_0=prior$nu_0, beta_0=prior$beta_0)
    mu_now <- update_mu(n=n, y=y, sig2=sig2_now, mu_0=prior$mu_0, sig2_0=prior$sig2_0)
    
    sig2_out[i] <- sig2_now
    mu_out[i] <- mu_now
    
  }
  
  cbind(mu=mu_out, sig2=sig2_out)
}
```

## Sample from Posterior

Now, we do as before, we load in the data and do the sampling.

```{r}
set.seed(42)
y <- c(1.2, 1.4, -0.5, 0.3, 0.9, 2.3, 1.0, 0.1, 1.3, 1.9)
n <- length(y)

```

We also need to pass a prior, so let's set that now.

```{r}
prior <- list()

prior$n_0 <- 2.0
prior$s2_0 <- 1.0

prior$mu_0 <- 0.0
prior$sig2_0 <- 1.0

prior$nu_0 <- prior$n_0 / 2.0
prior$beta_0 <- prior$n_0 * prior$s2_0 / 2.0

```

In addition to the priors, we also need initial values

```{r}
init <- list()

init$mu <- 0.0

```

Now to run our Gibbs Sampler

```{r}
post <- gibbs(y=y, n_iter=1e3, init=init, prior=prior)

head(post)
```

## Post Processing

Now that we have our samples from the posterior, let's take a look at what we found.

```{r}
plot(as.mcmc(post))
```

```{r}
summary(as.mcmc(post))
```

We find similar results to before. Noteworty is that although 1 was a good approximation for sigma squared, the maximum posteriori would have been a little lower than that. 
